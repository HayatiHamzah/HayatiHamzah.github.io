<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>
<link href="https://gmpg.org/xfn/11" rel="profile">
<meta http-equiv="content-type" content="text/html; charset=utf-8">

<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
<meta name="description" content="Open-Source Deep-Learning Software for Java and Scala on Hadoop and Spark">
<meta name="author" content="Chris V. Nicholson, Adam Gibson, Skymind team">
<title>
    
      A Beginner's Guide to Deep Convolutional Networks - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM
    
  </title>

<link href="../assets/themes/thedocs/css/theDocs.all.min.css" rel="stylesheet">
<link href="../assets/themes/thedocs/css/skin-dark.css" rel="stylesheet">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.5.2/animate.min.css">

<link href="../assets/styles/dl4j.css" rel="stylesheet">

<link href="https://fonts.googleapis.com/css?family=Raleway:100,300,400,500%7CLato:300,400" rel='stylesheet' type='text/css'>

<link rel="apple-touch-icon-precomposed" sizes="144x144" href="../assets/themes/thedocs/img/apple-touch-icon-precomposed.png">
<link rel="shortcut icon" href="../assets/themes/thedocs/img/favicon.ico">

<link rel="alternate" type="application/rss+xml" title="RSS" href="atom.xml">

<script async defer src="https://buttons.github.io/buttons.js"></script>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-48811288-1', 'auto');
  ga('require', 'linkid', 'linkid.js');
  ga('send', 'pageview');
  ga('require', 'displayfeatures');
  </script>


<script type="text/javascript">
  setTimeout(function(){var a=document.createElement("script");
  var b=document.getElementsByTagName("script")[0];
  a.src=document.location.protocol+"//script.crazyegg.com/pages/scripts/0025/5605.js?"+Math.floor(new Date().getTime()/3600000);
  a.async=true;a.type="text/javascript";b.parentNode.insertBefore(a,b)}, 1);
  </script>


<script type="text/javascript" id="hs-script-loader" async defer src="//js.hs-scripts.com/2179705.js"></script>


<script type="text/javascript">
  piAId = '457082';
  piCId = '66281';
  piHostname = 'pi.pardot.com';

  (function() {
  	function async_load(){
  		var s = document.createElement('script'); s.type = 'text/javascript';
  		s.src = ('https:' == document.location.protocol ? 'https://pi' : 'http://cdn') + '.pardot.com/pd.js';
  		var c = document.getElementsByTagName('script')[0]; c.parentNode.insertBefore(s, c);
  	}
  	if(window.attachEvent) { window.attachEvent('onload', async_load); }
  	else { window.addEventListener('load', async_load, false); }
  })();
  </script>


<script src="https://cdn.optimizely.com/js/2296590312.js"></script>



</head>
<body>

<script type="text/javascript">
    var _kiq = _kiq || [];
    (function(){
      setTimeout(function(){
      var d = document, f = d.getElementsByTagName('script')[0], s = d.createElement('script'); s.type = 'text/javascript';
      s.async = true; s.src = '//s3.amazonaws.com/ki.js/68133/geH.js'; f.parentNode.insertBefore(s, f);
      }, 1);
    })();
  </script>
<style>
  /* Fix subscribe form formatting */
  li.subscribe-form {
    padding: 2px 20px !important;
  }

  .hs-richtext {
    margin-bottom: -30px;
  }

  .hs-button {
    background-color: #2196f3;
    border-color: #2196f3;
    color: #fff;
    margin-top: 10px;
  }

</style>

<header class="site-header navbar-fullwidth">

<nav class="navbar navbar-default">
<div class="container">

<div class="navbar-header">
<button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#navbar" aria-expanded="true" aria-controls="navbar">
<span class="glyphicon glyphicon-option-vertical"></span>
</button>
<button type="button" class="navbar-toggle for-sidebar" data-toggle="offcanvas">
<span class="icon-bar"></span>
<span class="icon-bar"></span>
<span class="icon-bar"></span>
</button>
<a class="navbar-brand" href="../index.html"><img src="../assets/themes/thedocs/img/DL4J.png" alt="Deeplearning4j"></a>
</div>


<div id="navbar" class="navbar-collapse collapse" aria-expanded="true" role="banner">
<ul class="nav navbar-nav navbar-right">
<li class="hero"><a href="quickstart">Quickstart</a></li>
<li><a href="documentation">Documentation</a></li>
<li><a href="gpu">GPUs</a></li>
<li><a href="spark">Spark</a></li>
<li><a href="lstm">LSTM</a></li>
<li><a href="about">About</a></li>
<li><a href="http://newsletter.deeplearning4j.org/l/456082/2017-12-06/dxd853" target="_blank">Newsletter</a></li>
</ul>
</div>
<div class="github-ribbon">
<a href="https://github.com/deeplearning4j/deeplearning4j"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://camo.githubusercontent.com/e7bbb0521b397edbd5fe43e7f760759336b5e05f/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f72696768745f677265656e5f3030373230302e706e67" alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_right_green_007200.png"></a>
</div>

</div>
</nav>

</header>


<aside class="sidebar sidebar-boxed sidebar-dark">
<a class="sidebar-brand" href="../index.html"><img src="../assets/themes/thedocs/img/DL4J.png" alt="Deeplearning4j"></a>
<ul class="sidenav dropable sticky">
<li><a href="https://www.amazon.com/Deep-Learning-Practitioners-Adam-Gibson/dp/1491914254" target="_blank">Deep Learning Textbook</a></li>
<li><a href="https://skymind.ai/platform" target="_blank">Download SKIL Community Edition</a></li>
<li><a href="https://skymind.ai/training" target="_blank">Request Corporate Training</a></li>
<li>
<a href="#">Getting Started</a>
<ul>
<li><a href="/overview">DeepLearning4J Overview</a></li>
<li><a href="/quickstart">Quickstart: Running DL4J</a></li>
<li><a href="/core-concepts">DeepLearning4J: Core Concepts</a></li>
<li><a href="/gettingstarted">Comprehensive Setup Guide</a></li>
<li><a href="/quickref">Quick Reference: Layers & Functionality<a></li>
<li><a href="/buildinglocally">Build Locally From Master</a></li>
<li><a href="/maven">Use the Maven Build Tool</a></li>
<li><a href="/buildtools">Or Configure DL4J in Ivy, Gradle, SBT etc.</a></li>
<li><a href="http://nd4j.org/gpu_native_backends.html">Swap CPUs for GPUs</a></li>
<li><a href="/benchmark">DeepLearning4J Benchmarks</a></li>
</ul>
</li>
<li>
<a href="#">Tutorials</a>
<ul>
<li><a href="/tutorials">Deep Learning Tutorial Index</a></li>
<li><a href="/mnist-for-beginners">MNIST for Beginners</a></li>
<li><a href="/usingrnns">Using Recurrent Nets in DL4J</a></li>
<li><a href="http://nd4j.org/userguide">Use ND4J for Scientific Computing</a></li>
<li><a href="/datavec">DataVec: Vectorization and Preprocessing for Machine Learning</a></li>
<li><a href="/updater">Neural Net Updaters: SGD, Adam, Adagrad, Adadelta, RMSProp</a></li>
<li><a href="/welldressed-recommendation-engine">Build a Recommendation Engine With DL4J</a></li>
<li><a href="/build_vgg_webapp">Build a Web Application for Image Classification</a></li>
<li><a href="/android">Deploy Deeplearning4j to Android</a></li>
</ul>
</li>
<li>
<a href="#">Introduction to Deep Learning</a>
<ul>
<li><a href="/neuralnet-overview">Introduction to Neural Networks</a></li>
<li><a href="/questions">Questions to Ask When Applying DL</a></li>
<li><a href="/deeplearningforbeginners.html">Deep Learning for Beginners</a></li>
<li><a href="/use_cases">Deep Learning Use Cases</a></li>
<li><a href="/accuracy">Deep Learning's Accuracy</a></li>
<li><a href="/ai-machinelearning-deeplearning">AI, Machine Learning and Deep Learning</a></li>
<li><a href="/data-for-deep-learning.html">The Data You Need For Deep Learning</a></li>
<li><a href="/multinetwork">Multilayer Neural Nets</a></li>
<li><a href="/neuralnetworktable">Choosing a Neural Network</a></li>
</ul>
</li>
<li>
<a href="#">Neural Networks</a>
<ul>
<li><a href="/lstm">Long Short-Term Memory Units</a></li>
<li><a href="/convolutionalnets">Convolutional Nets for Image Processing</a></li>
<li><a href="/recurrentnetwork">Recurrent Nets and LSTMs</a></li>
<li><a href="/word2vec">Word2Vec: Neural Word Embeddings</a></li>
<li><a href="/restrictedboltzmannmachine">Restricted Boltzmann Machines</a></li>
<li><a href="/deepautoencoder">Deep AutoEncoder</a></li>
<li><a href="/denoisingautoencoder">Denoising Autoencoders</a></li>
<li><a href="/stackeddenoisingautoencoder">Stacked Denoising Autoencoders</a></li>
<li><a href="/building-neural-net-with-dl4j">Building a Neural Net with DeepLearning4J</a></li>
<li><a href="/evaluation">Evaluating Neural Nets</a></li>
</ul>
</li>
<li>
<a href="#">Data & ETL</a>
<ul>
<li><a href="/etl-userguide">ETL User Guide</a></li>
<li><a href="/datavec">DataVec: ETL for Machine Learning</a></li>
<li><a href="/workspaces">Workspaces</a></li>
<li><a href="/image-data-pipeline.html#record">Build a Data Pipeline</a></li>
<li><a href="/simple-image-load-transform">Customize an Image Pipeline</a></li>
<li><a href="/datavecdoc/">DataVec Javadoc: DataVec Methods & Classes for ETL</a></li>
<li><a href="/data-sets-ml">Datasets and Machine Learning</a></li>
<li><a href="/customdatasets">Custom Datasets</a></li>
<li><a href="/csv-deep-learning">CSV Data Uploads</a></li>
<li><a href="/opendata">Open Data</a></li>
</ul>
</li>
<li>
<a href="#">Tuning & Training</a>
<ul>
<li><a href="/memory">Memory management options</a></li>
<li><a href="/spark">Training Neural Networks with Apache Spark</a></li>
<li><a href="/distributed">Apache Spark & DL4J Parameter Server</a></li>
<li><a href="/iterativereduce">Distributed Training: Iterative Reduce Defined</a></li>
<li><a href="/visualization">Visualize, Monitor and Debug Network Learning</a></li>
<li><a href="/troubleshootingneuralnets">Troubleshoot Training & Select Network Hyperparameters</a></li>
<li><a href="/earlystopping">Train Networks using Early Stopping</a></li>
<li><a href="/output">Interpret Neural Net Output</a></li>
</ul>
</li>
<li>
<a href="#">Advanced Usage</a>
<ul>
<li><a href="/spark-gpus">Running Deep Learning on Distributed GPUs With Spark</a></li>
<li><a href="/model-zoo">Model Zoo: Pre-trained Models</a></li>
<li><a href="/modelpersistence">Save and Load Models</a></li>
<li><a href="/tsne-visualization">Visualize Data with t-SNE</a></li>
<li><a href="/linear-regression">Perform Regression With Neural Nets</a></li>
<li><a href="/usingrnns">Use Recurrent Networks in DL4J</a></li>
<li><a href="/compgraph">Build Complex Network Architectures with Computation Graph</a></li>
</ul>
</li>
<li>
<a href="#">Open-Source Community</a>
<ul>
<li><a href="/devguide">Contribute to DL4J (Developer Guide)</a></li>

<li><a href="/features">Features</a></li>
<li><a href="/roadmap">Roadmap</a></li>
<li><a href="/releasenotes">Latest Release Notes</a></li>
<li><a href="/doc">Javadoc: DL4J Methods and Classes</a></li>
</ul>
</li>
<li>
<a href="#">Natural Language Processing</a>
<ul>
<li><a href="/nlp">DL4J's NLP Functionality</a></li>
<li><a href="/word2vec">Word2vec for Java and Scala</a></li>
<li><a href="/doc2vec">Doc2vec for Java and Scala</a></li>
<li><a href="/textanalysis">Textual Analysis and DL</a></li>
<li><a href="/bagofwords-tf-idf">Bag of Words</a></li>
<li><a href="/sentenceiterator">Sentence and Document Segmentation</a></li>
<li><a href="/tokenization">Tokenization</a></li>
<li><a href="/vocabcache">Vocab Cache</a></li>
</ul>
</li>
<li>
<a href="#">ND4J: Numpy for the JVM</a>
<ul>
<li><a href="http://nd4j.org/backend.html">ND4J Backends: Hardware Acceleration on CPUs and GPUs</a></li>
<li><a href="http://nd4j.org/userguide">ND4J User Guide</a></li>
<li><a href="http://nd4j.org/doc/">ND4J Javadoc</a></li>
<li><a href="https://deeplearning4j.org/jumpy">Jumpy: Numpy Arrays for the JVM</a></li>
</ul>
</li>
<li>
<a href="#">Resources</a>
<ul>
<li><a href="/eigenvector">Eigenvectors, PCA, Covariance and Entropy</a></li>
<li><a href="/thoughtvectors">Thought Vectors, AI and NLP</a></li>
<li><a href="/markovchainmontecarlo">Monte Carlo, Markov Chains and Deep Learning</a></li>
<li><a href="/unsupervised-learning">Unsupervised Learning: Use Cases</a></li>
<li><a href="/reinforcementlearning">DL and Reinforcement Learning</a></li>
<li><a href="/scala">Scala, Spark and Deep Learning</a></li>
<li><a href="/compare-dl4j-torch7-pylearn">DL4J, Torch7, Theano and Caffe</a></li>
<li><a href="/glossary">Glossary of Terms for Deep Learning and Neural Nets</a></li>
<li><a href="/deeplearningpapers">Free Online Courses, Tutorials and Papers</a></li>
<li><a href="/deeplearningtranslated">Deep Learning in Other Languages</a></li>
</ul>
</li>
<li>
<a href="#">Other Languages</a>
<ul>
<li><a href="../cn/index">中文</a></li>
<li><a href="../ja-index">日本語</a></li>
<li><a href="../kr-index">한글</a></li>
</ul>
</li>
</ul>
</aside>

<main class="container-fluid">
<div class="row">

<article class="main-content" role="main">
<h1 id="a-beginners-guide-to-deep-convolutional-networks">A Beginner’s Guide to Deep Convolutional Networks</h1>
<p>Contents</p>
<ul>
<li><a href="#intro">Deep Convolutional Network Introduction</a></li>
<li><a href="#tensors">Images Are 4-D Tensors?</a></li>
<li><a href="#define">ConvNet Definition</a></li>
<li><a href="#work">How Deep Convolutional Nets Work</a></li>
<li><a href="#max">Maxpooling/Downsampling</a></li>
<li><a href="#code">DL4J Code Sample</a></li>
<li><a href="#resource">Other Resources</a></li>
</ul>
<h2 id="introduction-to-deep-convolutional-networks"><a name="intro">Introduction to Deep Convolutional Networks</a></h2>
<p>Convolutional networks are deep artificial neural networks that can be used to classify images (name what they see), cluster them by similarity (photo search), and perform object recognition within scenes. They are algorithms that can identify faces, individuals, street signs, eggplants, platypuses and many other aspects of visual data.</p>
<p>Convolutional networks perform optical character recognition (OCR) to digitize text and make natural-language processing possible on analog and hand-written documents, where the images are symbols to be transcribed. CNNs can also be applied to sound when it is represented visually as a spectrogram. More recently, convolutional networks have been applied directly to <a href="http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/">text analytics</a> as well as graph data with <a href="./graphdata">graph convolutional networks</a>.</p>
<p>The efficacy of convolutional nets (ConvNets or CNNs) in image recognition is one of the main reasons why the world has woken up to the efficacy of deep learning. They are powering major advances in machine vision, which has obvious applications for self-driving cars, robotics, drones, security, medical diagnoses, and treatments for the visually impaired.</p>
<p align="center">
<a href="https://docs.skymind.ai/docs/welcome" type="button" class="btn btn-lg btn-success" onclick="ga('send', 'event', ‘quickstart', 'click');">GET STARTED WITH CONVOLUTIONAL NETWORKS</a>
</p>
<h2 id="images-are-4-d-tensors"><a name="tensors">Images Are 4-D Tensors?</a></h2>
<p>Convolutional nets ingest and process images as tensors, and tensors are matrices of numbers with additional dimensions.</p>
<p>They can be hard to visualize, so let’s approach them by analogy. A scalar is just a number, such as 7; a vector is a list of numbers (e.g., <code class="highlighter-rouge">[7,8,9]</code>); and a matrix is a rectangular grid of numbers occupying several rows and columns like a spreadsheet. Geometrically, if a scalar is a zero-dimensional point, then a vector is a one-dimensional line, a matrix is a two-dimensional plane, a stack of matrices is a three-dimensional cube, and when each element of those matrices has a stack of <em>feature maps</em> atttached to it, you enter the fourth dimension. For reference, here’s a 2 x 2 matrix:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[ 1, 2 ] 
[ 5, 8 ]
</code></pre></div></div>
<p>A tensor encompasses the dimensions beyond that 2-D plane. You can easily picture a three-dimensional tensor, with the array of numbers arranged in a cube. Here’s a 2 x 3 x 2 tensor presented flatly (picture the bottom element of each 2-element array extending along the z-axis to intuitively grasp why it’s called a 3-dimensional array):</p>
<p><img src="/img/tensor.png" alt="Alt text" /></p>
<p>In code, the tensor above would appear like this: <code class="highlighter-rouge">[[[2,3],[3,5],[4,7]],[[3,4],[4,6],[5,8]]].</code> And here’s a visual:</p>
<p><img src="/img/3d_matrix_cube.png" alt="Alt text" /></p>
<p>In other words, tensors are formed by arrays nested within arrays, and that nesting can go on infinitely, accounting for an arbitrary number of dimensions far greater than what we can visualize spatially. A 4-D tensor would simply replace each of these scalars with an array nested one level deeper. Convolutional networks deal in 4-D tensors like the one below (notice the nested array).</p>
<p><img src="/img/3d_matrix.png" alt="Alt text" /></p>
<p>ND4J and Deeplearning4j use <code class="highlighter-rouge">NDArray</code> synonymously with tensor. A tensor’s dimensionality <code class="highlighter-rouge">(1,2,3…n)</code> is called its order; i.e. a fifth-order tensor would have five dimensions.</p>
<p>The width and height of an image are easily understood. The depth is necessary because of how colors are encoded. Red-Green-Blue (RGB) encoding, for example, produces an image three layers deep. Each layer is called a “channel”, and through convolution it produces a stack of feature maps (explained below), which exist in the fourth dimension, just down the street from time itself. (Features are just details of images, like a line or curve, that convolutional networks create maps of.)</p>
<p>So instead of thinking of images as two-dimensional areas, in convolutional nets they are treated as four-dimensional volumes. These ideas will be explored more thoroughly below.</p>
<h2 id="definition"><a name="define">Definition</a></h2>
<p>From the Latin <em>convolvere</em>, “to convolve” means to roll together. For mathematical purposes, a convolution is the integral measuring how much two functions overlap as one passes over the other. Think of a convolution as a way of mixing two functions by multiplying them.</p>

<iframe src="img/convgaus.gif" width="100%" height="260px;" style="border:none;"></iframe>
<p><em>Credit: <a href="http://mathworld.wolfram.com/">Mathworld</a>. “The green curve shows the convolution of the blue and red curves as a function of t, the position indicated by the vertical green line. The gray region indicates the product <code class="highlighter-rouge">g(tau)f(t-tau)</code> as a function of t, so its area as a function of t is precisely the convolution.”</em></p>
<p>Look at the tall, narrow bell curve standing in the middle of a graph. The integral is the area under that curve. Near it is a second bell curve that is shorter and wider, drifting slowly from the left side of the graph to the right. The product of those two functions’ overlap at each point along the x-axis is their <a href="http://mathworld.wolfram.com/Convolution.html">convolution</a>. So in a sense, the two functions are being “rolled together.”</p>
<p>With image analysis, the static, underlying function (the equivalent of the immobile bell curve) is the input image being analyzed, and the second, mobile function is known as the filter, because it picks up a signal or feature in the image. The two functions relate through multiplication. To visualize convolutions as matrices rather than as bell curves, please see <a href="https://cs231n.github.io/convolutional-networks/">Andrej Karpathy’s excellent animation</a> under the heading “Convolution Demo.”</p>
<p>The next thing to understand about convolutional nets is that they are passing <em>many</em> filters over a single image, each one picking up a different signal. At a fairly early layer, you could imagine them as passing a horizontal line filter, a vertical line filter, and a diagonal line filter to create a map of the edges in the image.</p>
<p>Convolutional networks take those filters, slices of the image’s feature space, and map them one by one; that is, they create a map of each place that feature occurs. By learning different portions of a feature space, convolutional nets allow for easily scalable and robust feature engineering.</p>
<p>(Note that convolutional nets analyze images differently than RBMs. While RBMs learn to reconstruct and identify the features of each image as a whole, convolutional nets learn images in pieces that we call feature maps.)</p>
<p>So convolutional networks perform a sort of search. Picture a small magnifying glass sliding left to right across a larger image, and recommencing at the left once it reaches the end of one pass (like typewriters do). That moving window is capable recognizing only one thing, say, a short vertical line. Three dark pixels stacked atop one another. It moves that vertical-line-recognizing filter over the actual pixels of the image, looking for matches.</p>
<p>Each time a match is found, it is mapped onto a feature space particular to that visual element. In that space, the location of each vertical line match is recorded, a bit like birdwatchers leave pins in a map to mark where they last saw a great blue heron. A convolutional net runs many, many searches over a single image – horizontal lines, diagonal ones, as many as there are visual elements to be sought.</p>
<p>Convolutional nets perform more operations on input than just convolutions themselves.</p>
<p>After a convolutional layer, input is passed through a nonlinear transform such as <em>tanh</em> or <em>rectified linear</em> unit, which will squash input values into a range between -1 and 1.</p>
<h2 id="how-convolutional-networks-work"><a name="work">How Convolutional Networks Work</a></h2>
<p>The first thing to know about convolutional networks is that they don’t perceive images like humans do. Therefore, you are going to have to think in a different way about what an image means as it is fed to and processed by a convolutional network.</p>
<p>Convolutional networks perceive images as volumes; i.e. three-dimensional objects, rather than flat canvases to be measured only by width and height. That’s because digital color images have a red-blue-green (RGB) encoding, mixing those three colors to produce the color spectrum humans perceive. A convolutional network ingests such images as three separate strata of color stacked one on top of the other.</p>
<p>So a convolutional network receives a normal color image as a rectangular box whose width and height are measured by the number of pixels along those dimensions, and whose depth is three layers deep, one for each letter in RGB. Those depth layers are referred to as <em>channels</em>.</p>
<p>As images move through a convolutional network, we will describe them in terms of input and output volumes, expressing them mathematically as matrices of multiple dimensions in this form: 30x30x3. From layer to layer, their dimensions change for reasons that will be explained below.</p>
<p>You will need to pay close attention to the precise measures of each dimension of the image volume, because they are the foundation of the linear algebra operations used to process images.</p>
<p>Now, for each pixel of an image, the intensity of R, G and B will be expressed by a number, and that number will be an element in one of the three, stacked two-dimensional matrices, which together form the image volume.</p>
<p>Those numbers are the initial, raw, sensory features being fed into the convolutional network, and the ConvNets purpose is to find which of those numbers are significant signals that actually help it classify images more accurately. (Just like other feedforward networks we have discussed.)</p>
<p>Rather than focus on one pixel at a time, a convolutional net takes in square patches of pixels and passes them through a <em>filter</em>. That filter is also a square matrix smaller than the image itself, and equal in size to the patch. It is also called a <em>kernel</em>, which will ring a bell for those familiar with support-vector machines, and the job of the filter is to find patterns in the pixels.</p>
<iframe src="https://cs231n.github.io/assets/conv-demo/index.html" width="100%" height="700px;" style="border:none;"></iframe>
<p><em>Credit for this excellent animation goes to <a href="https://cs231n.github.io/">Andrej Karpathy</a>.</em></p>
<p>Imagine two matrices. One is 30x30, and another is 3x3. That is, the filter covers one-hundredth of one image channel’s surface area.</p>
<p>We are going to take the dot product of the filter with this patch of the image channel. If the two matrices have high values in the same positions, the dot product’s output will be high. If they don’t, it will be low. In this way, a single value – the output of the dot product – can tell us whether the pixel pattern in the underlying image matches the pixel pattern expressed by our filter.</p>
<p>Let’s imagine that our filter expresses a horizontal line, with high values along its second row and low values in the first and third rows. Now picture that we start in the upper lefthand corner of the underlying image, and we move the filter across the image step by step until it reaches the upper righthand corner. The size of the step is known as <em>stride</em>. You can move the filter to the right one column at a time, or you can choose to make larger steps.</p>
<p>At each step, you take another dot product, and you place the results of that dot product in a third matrix known as an <em>activation map</em>. The width, or number of columns, of the activation map is equal to the number of steps the filter takes to traverse the underlying image. Since larger strides lead to fewer steps, a big stride will produce a smaller activation map. This is important, because the size of the matrices that convolutional networks process and produce at each layer is directly proportional to how computationally expensive they are and how much time they take to train. A larger stride means less time and compute.</p>
<p>A filter superimposed on the first three rows will slide across them and then begin again with rows 4-6 of the same image. If it has a stride of three, then it will produce a matrix of dot products that is 10x10. That same filter representing a horizontal line can be applied to all three channels of the underlying image, R, G and B. And the three 10x10 activation maps can be added together, so that the aggregate activation map for a horizontal line on all three channels of the underlying image is also 10x10.</p>
<p>Now, because images have lines going in many directions, and contain many different kinds of shapes and pixel patterns, you will want to slide other filters across the underlying image in search of those patterns. You could, for example, look for 96 different patterns in the pixels. Those 96 patterns will create a stack of 96 activation maps, resulting in a new volume that is 10x10x96. In the diagram below, we’ve relabeled the input image, the kernels and the output activation maps to make sure we’re clear.</p>
<p><img src="/img/karpathy-convnet-labels.png" alt="Alt text" /></p>
<p>What we just described is a convolution. You can think of Convolution as a fancy kind of multiplication used in signal processing. Another way to think about the two matrices creating a dot product is as two functions. The image is the underlying function, and the filter is the function you roll over it.</p>
<iframe src="https://mathworld.wolfram.com//images/gifs/convgaus.gif" width="100%" height="250px;" style="border:none;"></iframe>
<p>One of the main problems with images is that they are high-dimensional, which means they cost a lot of time and computing power to process. Convolutional networks are designed to reduce the dimensionality of images in a variety of ways. Filter stride is one way to reduce dimensionality. Another way is through downsampling.</p>
<h2 id="max-poolingdownsampling"><a name="max">Max Pooling/Downsampling</a></h2>
<p>The next layer in a convolutional network has three names: max pooling, downsampling and subsampling. The activation maps are fed into a downsampling layer, and like convolutions, this method is applied one patch at a time. In this case, max pooling simply takes the largest value from one patch of an image, places it in a new matrix next to the max values from other patches, and discards the rest of the information contained in the activation maps.</p>
<p><img src="/img/maxpool.png" alt="Alt text" />
<em>Credit to <a href="https://cs231n.github.io/">Andrej Karpathy</a>.</em></p>
<p>Only the locations on the image that showed the strongest correlation to each feature (the maximum value) are preserved, and those maximum values combine to form a lower-dimensional space.</p>
<p>Much information about lesser values is lost in this step, which has spurred research into alternative methods. But downsampling has the advantage, precisely because information is lost, of decreasing the amount of storage and processing required.</p>
<h3 id="alternating-layers">Alternating Layers</h3>
<p>The image below is another attempt to show the sequence of transformations involved in a typical convolutional network.</p>
<p><img src="/img/convnet.png" alt="Alt text" /></p>
<p>From left to right you see:</p>
<ul>
<li>The actual input image that is scanned for features. The light rectangle is the filter that passes over it.</li>
<li>Activation maps stacked atop one another, one for each filter you employ. The larger rectangle is one patch to be downsampled.</li>
<li>The activation maps condensed through downsampling.</li>
<li>A new set of activation maps created by passing filters over the first downsampled stack.</li>
<li>The second downsampling, which condenses the second set of activation maps.</li>
<li>A fully connected layer that classifies output with one label per node.</li>
</ul>
<p>As more and more information is lost, the patterns processed by the convolutional net become more abstract and grow more distant from visual patterns we recognize as humans. So forgive yourself, and us, if convolutional networks do not offer easy intuitions as they grow deeper.</p>
<h2 id="dl4j-code-example"><a name="code">DL4J Code Example</a></h2>
<p>Here’s one example of how you might configure a ConvNet with Deeplearning4j:</p>
<script src="https://gist-it.appspot.com/https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/convolution/LenetMnistExample.java?slice=43:87"></script>
<p>All Deeplearning4j <a href="https://github.com/deeplearning4j/dl4j-examples/tree/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/convolution">examples of convolutional networks are available here</a>.</p>
<h3 id="other-machine-learning-tutorials"><a name="beginner">Other Machine Learning Tutorials</a></h3>
<ul>
<li><a href="./neuralnet-overview.html">Introduction to Neural Networks</a></li>
<li><a href="./deepreinforcementlearning.html">Deep Reinforcement Learning</a></li>
<li><a href="./convolutionalnets.html">Convolutional Networks</a></li>
<li><a href="./lstm.html">Recurrent Networks and LSTMs</a></li>
<li><a href="./multilayerperceptron.html">Multilayer Perceptron (MLPs) for Classification</a></li>
<li><a href="./generative-adversarial-network.html">Generative Adversarial Networks (GANs)</a></li>
<li><a href="./symbolicreasoning.html">Symbolic Reasoning &amp; Deep Learning</a></li>
<li><a href="./graphdata.html">Using Graph Data with Deep Learning</a></li>
<li><a href="./ai-machinelearning-deeplearning.html">AI vs. Machine Learning vs. Deep Learning</a></li>
<li><a href="/mnist-for-beginners">MNIST for Beginners</a></li>
<li><a href="./restrictedboltzmannmachine.html">Restricted Boltzmann Machines</a></li>
<li><a href="./eigenvector.html">Eigenvectors, PCA, Covariance and Entropy</a></li>
<li><a href="./glossary.html">Glossary of Deep-Learning and Neural-Net Terms</a></li>
<li><a href="./word2vec.html">Word2vec and Natural-Language Processing</a></li>
<li><a href="./quickstart.html">Deeplearning4j Examples via Quickstart</a></li>
<li><a href="https://www.youtube.com/watch?v=bxe2T-V8XRs">Neural Networks Demystified</a> (A seven-video series)</li>
<li><a href="./modelserver.html">Inference: Machine Learning Model Server</a></li>
<li><a href="./ai-machinelearning-deeplearning.html">AI vs. Machine Learning vs. Deep Learning</a></li>
</ul>
<h2 id="other-resources"><a name="resource">Other Resources</a></h2>
<p>To see DL4J convolutional networks in action, please run our <a href="https://github.com/deeplearning4j/dl4j-examples/tree/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/convolution/">examples</a> after following the instructions on the <a href="./quickstart">Quickstart page</a>.</p>
<p>Skymind wraps NVIDIA’s cuDNN and integrates with OpenCV. Our convolutional nets run on distributed GPUs using Spark, making them among the fastest in the world. You can learn how to build a <a href="./build_vgg_webapp">image recognition web app with VGG16 here</a> and how to <a href="./android">deploy CNNs to Android here</a>.</p>
<h2 id="resources-table-of-contents">Resources Table of Contents</h2>
<ul>
<li><a href="#papers">Papers</a>
<ul>
<li><a href="#imagenet-classification">ImageNet Classification</a></li>
<li><a href="#object-detection">Object Detection</a></li>
<li><a href="#object-tracking">Object Tracking</a></li>
<li><a href="#low-level-vision">Low-Level Vision</a>
<ul>
<li><a href="#super-resolution">Super-Resolution</a></li>
<li><a href="#other-applications">Other Applications</a></li>
</ul>
</li>
<li><a href="#edge-detection">Edge Detection</a></li>
<li><a href="#semantic-segmentation">Semantic Segmentation</a></li>
<li><a href="#visual-attention-and-saliency">Visual Attention and Saliency</a></li>
<li><a href="#object-recognition">Object Recognition</a></li>
<li><a href="#human-pose-estimation">Human Pose Estimation</a></li>
<li><a href="#understanding-cnn">Understanding CNN</a></li>
<li><a href="#image-and-language">Image and Language</a>
<ul>
<li><a href="#image-captioning">Image Captioning</a></li>
<li><a href="#video-captioning">Video Captioning</a></li>
<li><a href="#question-answering">Question Answering</a></li>
</ul>
</li>
<li><a href="#image-generation">Image Generation</a></li>
<li><a href="#other-topics">Other Topics</a></li>
</ul>
</li>
<li><a href="#courses">Courses</a></li>
<li><a href="#books">Books</a></li>
<li><a href="#videos">Videos</a></li>
<li><a href="#software">Software</a>
<ul>
<li><a href="#framework">Framework</a></li>
<li><a href="#applications">Applications</a></li>
</ul>
</li>
<li><a href="#tutorials">Tutorials</a></li>
<li><a href="#blogs">Blogs</a></li>
</ul>
<h2 id="papers">Papers</h2>
<h3 id="imagenet-classification">ImageNet Classification</h3>
<p><img src="https://cloud.githubusercontent.com/assets/5226447/8451949/327b9566-2022-11e5-8b34-53b4a64c13ad.PNG" alt="classification" />
(from Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton, ImageNet Classification with Deep Convolutional Neural Networks, NIPS, 2012.)</p>
<ul>
<li>Microsoft (Deep Residual Learning) [<a href="https://arxiv.org/pdf/1512.03385v1.pdf">Paper</a>][<a href="http://image-net.org/challenges/talks/ilsvrc2015_deep_residual_learning_kaiminghe.pdf">Slide</a>]
<ul>
<li>Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Deep Residual Learning for Image Recognition, arXiv:1512.03385.</li>
</ul>
</li>
<li>Microsoft (PReLu/Weight Initialization) <a href="https://arxiv.org/pdf/1502.01852">[Paper]</a>
<ul>
<li>Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification, arXiv:1502.01852.</li>
</ul>
</li>
<li>Batch Normalization <a href="https://arxiv.org/pdf/1502.03167">[Paper]</a>
<ul>
<li>Sergey Ioffe, Christian Szegedy, Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, arXiv:1502.03167.</li>
</ul>
</li>
<li>GoogLeNet <a href="https://arxiv.org/pdf/1409.4842">[Paper]</a>
<ul>
<li>Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich, CVPR, 2015.</li>
</ul>
</li>
<li>VGG-Net <a href="http://www.robots.ox.ac.uk/~vgg/research/very_deep/">[Web]</a> <a href="https://arxiv.org/pdf/1409.1556">[Paper]</a>
<ul>
<li>Karen Simonyan and Andrew Zisserman, Very Deep Convolutional Networks for Large-Scale Visual Recognition, ICLR, 2015.</li>
</ul>
</li>
<li>AlexNet <a href="http://papers.nips.cc/book/advances-in-neural-information-processing-systems-25-2012">[Paper]</a>
<ul>
<li>Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton, ImageNet Classification with Deep Convolutional Neural Networks, NIPS, 2012.</li>
</ul>
</li>
</ul>
<h3 id="object-detection">Object Detection</h3>
<p><img src="https://cloud.githubusercontent.com/assets/5226447/8452063/f76ba500-2022-11e5-8db1-2cd5d490e3b3.PNG" alt="object_detection" />
(from Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun, Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, arXiv:1506.01497.)</p>
<ul>
<li>PVANET <a href="https://arxiv.org/pdf/1608.08021">[Paper]</a> <a href="https://github.com/sanghoon/pva-faster-rcnn">[Code]</a>
<ul>
<li>Kye-Hyeon Kim, Sanghoon Hong, Byungseok Roh, Yeongjae Cheon, Minje Park, PVANET: Deep but Lightweight Neural Networks for Real-time Object Detection, arXiv:1608.08021</li>
</ul>
</li>
<li>OverFeat, NYU <a href="https://arxiv.org/pdf/1312.6229.pdf">[Paper]</a>
<ul>
<li>OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks, ICLR, 2014.</li>
</ul>
</li>
<li>R-CNN, UC Berkeley <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf">[Paper-CVPR14]</a> <a href="https://arxiv.org/pdf/1311.2524">[Paper-arXiv14]</a>
<ul>
<li>Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik, Rich feature hierarchies for accurate object detection and semantic segmentation, CVPR, 2014.</li>
</ul>
</li>
<li>SPP, Microsoft Research <a href="https://arxiv.org/pdf/1406.4729">[Paper]</a>
<ul>
<li>Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition, ECCV, 2014.</li>
</ul>
</li>
<li>Fast R-CNN, Microsoft Research <a href="https://arxiv.org/pdf/1504.08083">[Paper]</a>
<ul>
<li>Ross Girshick, Fast R-CNN, arXiv:1504.08083.</li>
</ul>
</li>
<li>Faster R-CNN, Microsoft Research <a href="https://arxiv.org/pdf/1506.01497">[Paper]</a>
<ul>
<li>Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun, Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, arXiv:1506.01497.</li>
</ul>
</li>
<li>R-CNN minus R, Oxford <a href="https://arxiv.org/pdf/1506.06981">[Paper]</a>
<ul>
<li>Karel Lenc, Andrea Vedaldi, R-CNN minus R, arXiv:1506.06981.</li>
</ul>
</li>
<li>End-to-end people detection in crowded scenes <a href="https://arxiv.org/abs/1506.04878">[Paper]</a>
<ul>
<li>Russell Stewart, Mykhaylo Andriluka, End-to-end people detection in crowded scenes, arXiv:1506.04878.</li>
</ul>
</li>
<li>You Only Look Once: Unified, Real-Time Object Detection <a href="https://arxiv.org/abs/1506.02640">[Paper]</a>, <a href="https://arxiv.org/abs/1612.08242">[Paper Version 2]</a>, <a href="https://github.com/pjreddie/darknet">[C Code]</a>, <a href="https://github.com/thtrieu/darkflow">[Tensorflow Code]</a>
<ul>
<li>Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi, You Only Look Once: Unified, Real-Time Object Detection, arXiv:1506.02640</li>
<li>Joseph Redmon, Ali Farhadi (Version 2)</li>
</ul>
</li>
<li>Inside-Outside Net <a href="https://arxiv.org/abs/1512.04143">[Paper]</a>
<ul>
<li>Sean Bell, C. Lawrence Zitnick, Kavita Bala, Ross Girshick, Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks</li>
</ul>
</li>
<li>Deep Residual Network (Current State-of-the-Art) <a href="https://arxiv.org/abs/1512.03385">[Paper]</a>
<ul>
<li>Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Deep Residual Learning for Image Recognition</li>
</ul>
</li>
<li>Weakly Supervised Object Localization with Multi-fold Multiple Instance Learning [<a href="https://arxiv.org/pdf/1503.00949.pdf">Paper</a>]</li>
<li>R-FCN <a href="https://arxiv.org/abs/1605.06409">[Paper]</a> <a href="https://github.com/daijifeng001/R-FCN">[Code]</a>
<ul>
<li>Jifeng Dai, Yi Li, Kaiming He, Jian Sun, R-FCN: Object Detection via Region-based Fully Convolutional Networks</li>
</ul>
</li>
<li>SSD <a href="https://arxiv.org/pdf/1512.02325v2.pdf">[Paper]</a> <a href="https://github.com/weiliu89/caffe/tree/ssd">[Code]</a>
<ul>
<li>Wei Liu1, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg, SSD: Single Shot MultiBox Detector, arXiv:1512.02325</li>
</ul>
</li>
<li>Speed/accuracy trade-offs for modern convolutional object detectors <a href="https://arxiv.org/pdf/1611.10012v1.pdf">[Paper]</a>
<ul>
<li>Jonathan Huang, Vivek Rathod, Chen Sun, Menglong Zhu, Anoop Korattikara, Alireza Fathi, Ian Fischer, Zbigniew Wojna, Yang Song, Sergio Guadarrama, Kevin Murphy, Google Research, arXiv:1611.10012</li>
</ul>
</li>
</ul>
<h3 id="video-classification">Video Classification</h3>
<ul>
<li>Nicolas Ballas, Li Yao, Pal Chris, Aaron Courville, “Delving Deeper into Convolutional Networks for Learning Video Representations”, ICLR 2016. [<a href="https://arxiv.org/pdf/1511.06432v4.pdf">Paper</a>]</li>
<li>Michael Mathieu, camille couprie, Yann Lecun, “Deep Multi Scale Video Prediction Beyond Mean Square Error”, ICLR 2016. [<a href="https://arxiv.org/pdf/1511.05440v6.pdf">Paper</a>]</li>
</ul>
<h3 id="object-tracking">Object Tracking</h3>
<ul>
<li>Seunghoon Hong, Tackgeun You, Suha Kwak, Bohyung Han, Online Tracking by Learning Discriminative Saliency Map with Convolutional Neural Network, arXiv:1502.06796. <a href="https://arxiv.org/pdf/1502.06796">[Paper]</a></li>
<li>Hanxi Li, Yi Li and Fatih Porikli, DeepTrack: Learning Discriminative Feature Representations by Convolutional Neural Networks for Visual Tracking, BMVC, 2014. <a href="http://www.bmva.org/bmvc/2014/files/paper028.pdf">[Paper]</a></li>
<li>N Wang, DY Yeung, Learning a Deep Compact Image Representation for Visual Tracking, NIPS, 2013. <a href="http://winsty.net/papers/dlt.pdf">[Paper]</a></li>
<li>Chao Ma, Jia-Bin Huang, Xiaokang Yang and Ming-Hsuan Yang, Hierarchical Convolutional Features for Visual Tracking, ICCV 2015 [<a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Ma_Hierarchical_Convolutional_Features_ICCV_2015_paper.pdf">Paper</a>] [<a href="https://github.com/jbhuang0604/CF2">Code</a>]</li>
<li>Lijun Wang, Wanli Ouyang, Xiaogang Wang, and Huchuan Lu, Visual Tracking with fully Convolutional Networks, ICCV 2015 [<a href="http://202.118.75.4/lu/Paper/ICCV2015/iccv15_lijun.pdf">Paper</a>] [<a href="https://github.com/scott89/FCNT">Code</a>]</li>
<li>Hyeonseob Namand Bohyung Han, Learning Multi-Domain Convolutional Neural Networks for Visual Tracking, [<a href="https://arxiv.org/pdf/1510.07945.pdf">Paper</a>] [<a href="https://github.com/HyeonseobNam/MDNet">Code</a>] [<a href="http://cvlab.postech.ac.kr/research/mdnet/">Project Page</a>]</li>
</ul>
<h3 id="low-level-vision">Low-Level Vision</h3>
<h4 id="super-resolution">Super-Resolution</h4>
<ul>
<li>Iterative Image Reconstruction
<ul>
<li>Sven Behnke: Learning Iterative Image Reconstruction. IJCAI, 2001. <a href="http://www.ais.uni-bonn.de/behnke/papers/ijcai01.pdf">[Paper]</a></li>
<li>Sven Behnke: Learning Iterative Image Reconstruction in the Neural Abstraction Pyramid. International Journal of Computational Intelligence and Applications, vol. 1, no. 4, pp. 427-438, 2001. <a href="http://www.ais.uni-bonn.de/behnke/papers/ijcia01.pdf">[Paper]</a></li>
</ul>
</li>
<li>Super-Resolution (SRCNN) <a href="http://mmlab.ie.cuhk.edu.hk/projects/SRCNN.html">[Web]</a> <a href="http://personal.ie.cuhk.edu.hk/~ccloy/files/eccv_2014_deepresolution.pdf">[Paper-ECCV14]</a> <a href="https://arxiv.org/pdf/1501.00092.pdf">[Paper-arXiv15]</a>
<ul>
<li>Chao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang, Learning a Deep Convolutional Network for Image Super-Resolution, ECCV, 2014.</li>
<li>Chao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang, Image Super-Resolution Using Deep Convolutional Networks, arXiv:1501.00092.</li>
</ul>
</li>
<li>Very Deep Super-Resolution
<ul>
<li>Jiwon Kim, Jung Kwon Lee, Kyoung Mu Lee, Accurate Image Super-Resolution Using Very Deep Convolutional Networks, arXiv:1511.04587, 2015. <a href="https://arxiv.org/abs/1511.04587">[Paper]</a></li>
</ul>
</li>
<li>Deeply-Recursive Convolutional Network
<ul>
<li>Jiwon Kim, Jung Kwon Lee, Kyoung Mu Lee, Deeply-Recursive Convolutional Network for Image Super-Resolution, arXiv:1511.04491, 2015. <a href="https://arxiv.org/abs/1511.04491">[Paper]</a></li>
</ul>
</li>
<li>Casade-Sparse-Coding-Network
<ul>
<li>Zhaowen Wang, Ding Liu, Wei Han, Jianchao Yang and Thomas S. Huang, Deep Networks for Image Super-Resolution with Sparse Prior. ICCV, 2015. <a href="http://www.ifp.illinois.edu/~dingliu2/iccv15/iccv15.pdf">[Paper]</a> <a href="http://www.ifp.illinois.edu/~dingliu2/iccv15/">[Code]</a></li>
</ul>
</li>
<li>Perceptual Losses for Super-Resolution
<ul>
<li>Justin Johnson, Alexandre Alahi, Li Fei-Fei, Perceptual Losses for Real-Time Style Transfer and Super-Resolution, arXiv:1603.08155, 2016. <a href="https://arxiv.org/abs/1603.08155">[Paper]</a> <a href="http://cs.stanford.edu/people/jcjohns/papers/fast-style/fast-style-supp.pdf">[Supplementary]</a></li>
</ul>
</li>
<li>SRGAN
<ul>
<li>Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, Wenzhe Shi, Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network, arXiv:1609.04802v3, 2016. <a href="https://arxiv.org/pdf/1609.04802v3.pdf">[Paper]</a></li>
</ul>
</li>
<li>Others
<ul>
<li>Osendorfer, Christian, Hubert Soyer, and Patrick van der Smagt, Image Super-Resolution with Fast Approximate Convolutional Sparse Coding, ICONIP, 2014. <a href="http://brml.org/uploads/tx_sibibtex/281.pdf">[Paper ICONIP-2014]</a></li>
</ul>
</li>
</ul>
<h4 id="other-applications">Other Applications</h4>
<ul>
<li>Optical Flow (FlowNet) <a href="https://arxiv.org/pdf/1504.06852">[Paper]</a>
<ul>
<li>Philipp Fischer, Alexey Dosovitskiy, Eddy Ilg, Philip Häusser, Caner Hazırbaş, Vladimir Golkov, Patrick van der Smagt, Daniel Cremers, Thomas Brox, FlowNet: Learning Optical Flow with Convolutional Networks, arXiv:1504.06852.</li>
</ul>
</li>
<li>Compression Artifacts Reduction <a href="https://arxiv.org/pdf/1504.06993">[Paper-arXiv15]</a>
<ul>
<li>Chao Dong, Yubin Deng, Chen Change Loy, Xiaoou Tang, Compression Artifacts Reduction by a Deep Convolutional Network, arXiv:1504.06993.</li>
</ul>
</li>
<li>Blur Removal
<ul>
<li>Christian J. Schuler, Michael Hirsch, Stefan Harmeling, Bernhard Schölkopf, Learning to Deblur, arXiv:1406.7444 <a href="https://arxiv.org/pdf/1406.7444.pdf">[Paper]</a></li>
<li>Jian Sun, Wenfei Cao, Zongben Xu, Jean Ponce, Learning a Convolutional Neural Network for Non-uniform Motion Blur Removal, CVPR, 2015 <a href="https://arxiv.org/pdf/1503.00593">[Paper]</a></li>
</ul>
</li>
<li>Image Deconvolution <a href="http://lxu.me/projects/dcnn/">[Web]</a> <a href="http://lxu.me/mypapers/dcnn_nips14.pdf">[Paper]</a>
<ul>
<li>Li Xu, Jimmy SJ. Ren, Ce Liu, Jiaya Jia, Deep Convolutional Neural Network for Image Deconvolution, NIPS, 2014.</li>
</ul>
</li>
<li>Deep Edge-Aware Filter <a href="http://jmlr.org/proceedings/papers/v37/xub15.pdf">[Paper]</a>
<ul>
<li>Li Xu, Jimmy SJ. Ren, Qiong Yan, Renjie Liao, Jiaya Jia, Deep Edge-Aware Filters, ICML, 2015.</li>
</ul>
</li>
<li>Computing the Stereo Matching Cost with a Convolutional Neural Network <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Zbontar_Computing_the_Stereo_2015_CVPR_paper.pdf">[Paper]</a>
<ul>
<li>Jure Žbontar, Yann LeCun, Computing the Stereo Matching Cost with a Convolutional Neural Network, CVPR, 2015.</li>
</ul>
</li>
<li>Colorful Image Colorization Richard Zhang, Phillip Isola, Alexei A. Efros, ECCV, 2016 <a href="https://arxiv.org/pdf/1603.08511.pdf">[Paper]</a>, <a href="https://github.com/richzhang/colorization">[Code]</a></li>
<li>Ryan Dahl, <a href="http://tinyclouds.org/colorize/">[Blog]</a></li>
<li>Feature Learning by Inpainting<a href="https://arxiv.org/pdf/1604.07379v1.pdf">[Paper]</a><a href="https://github.com/pathak22/context-encoder">[Code]</a>
<ul>
<li>Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, Alexei A. Efros, Context Encoders: Feature Learning by Inpainting, CVPR, 2016</li>
</ul>
</li>
</ul>
<h3 id="edge-detection">Edge Detection</h3>
<p><img src="https://cloud.githubusercontent.com/assets/5226447/8452371/93ca6f7e-2025-11e5-90f2-d428fd5ff7ac.PNG" alt="edge_detection" />
(from Gedas Bertasius, Jianbo Shi, Lorenzo Torresani, DeepEdge: A Multi-Scale Bifurcated Deep Network for Top-Down Contour Detection, CVPR, 2015.)</p>
<ul>
<li>Holistically-Nested Edge Detection <a href="https://arxiv.org/pdf/1504.06375">[Paper]</a> <a href="https://github.com/s9xie/hed">[Code]</a>
<ul>
<li>Saining Xie, Zhuowen Tu, Holistically-Nested Edge Detection, arXiv:1504.06375.</li>
</ul>
</li>
<li>DeepEdge <a href="https://arxiv.org/pdf/1412.1123">[Paper]</a>
<ul>
<li>Gedas Bertasius, Jianbo Shi, Lorenzo Torresani, DeepEdge: A Multi-Scale Bifurcated Deep Network for Top-Down Contour Detection, CVPR, 2015.</li>
</ul>
</li>
<li>DeepContour <a href="http://mc.eistar.net/UpLoadFiles/Papers/DeepContour_cvpr15.pdf">[Paper]</a>
<ul>
<li>Wei Shen, Xinggang Wang, Yan Wang, Xiang Bai, Zhijiang Zhang, DeepContour: A Deep Convolutional Feature Learned by Positive-Sharing Loss for Contour Detection, CVPR, 2015.</li>
</ul>
</li>
</ul>
<h3 id="semantic-segmentation">Semantic Segmentation</h3>
<p><img src="https://cloud.githubusercontent.com/assets/5226447/8452076/0ba8340c-2023-11e5-88bc-bebf4509b6bb.PNG" alt="semantic_segmantation" />
(from Jifeng Dai, Kaiming He, Jian Sun, BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation, arXiv:1503.01640.)</p>
<ul>
<li>PASCAL VOC2012 Challenge Leaderboard (01 Sep. 2016)
<img src="https://cloud.githubusercontent.com/assets/3803777/18164608/c3678488-7038-11e6-9ec1-74a1542dce13.png" alt="VOC2012_top_rankings" />
(from PASCAL VOC2012 <a href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&amp;compid=6">leaderboards</a>)</li>
<li>SEC: Seed, Expand and Constrain
<ul>
<li>Alexander Kolesnikov, Christoph Lampert, Seed, Expand and Constrain: Three Principles for Weakly-Supervised Image Segmentation, ECCV, 2016. <a href="http://pub.ist.ac.at/~akolesnikov/files/ECCV2016/main.pdf">[Paper]</a> <a href="https://github.com/kolesman/SEC">[Code]</a></li>
</ul>
</li>
<li>Adelaide
<ul>
<li>Guosheng Lin, Chunhua Shen, Ian Reid, Anton van dan Hengel, Efficient piecewise training of deep structured models for semantic segmentation, arXiv:1504.01013. <a href="https://arxiv.org/pdf/1504.01013">[Paper]</a> (1st ranked in VOC2012)</li>
<li>Guosheng Lin, Chunhua Shen, Ian Reid, Anton van den Hengel, Deeply Learning the Messages in Message Passing Inference, arXiv:1508.02108. <a href="https://arxiv.org/pdf/1506.02108">[Paper]</a> (4th ranked in VOC2012)</li>
</ul>
</li>
<li>Deep Parsing Network (DPN)
<ul>
<li>Ziwei Liu, Xiaoxiao Li, Ping Luo, Chen Change Loy, Xiaoou Tang, Semantic Image Segmentation via Deep Parsing Network, arXiv:1509.02634 / ICCV 2015 <a href="https://arxiv.org/pdf/1509.02634.pdf">[Paper]</a> (2nd ranked in VOC 2012)</li>
</ul>
</li>
<li>CentraleSuperBoundaries, INRIA <a href="https://arxiv.org/pdf/1511.07386">[Paper]</a>
<ul>
<li>Iasonas Kokkinos, Surpassing Humans in Boundary Detection using Deep Learning, arXiv:1411.07386 (4th ranked in VOC 2012)</li>
</ul>
</li>
<li>BoxSup <a href="https://arxiv.org/pdf/1503.01640">[Paper]</a>
<ul>
<li>Jifeng Dai, Kaiming He, Jian Sun, BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation, arXiv:1503.01640. (6th ranked in VOC2012)</li>
</ul>
</li>
<li>POSTECH
<ul>
<li>Hyeonwoo Noh, Seunghoon Hong, Bohyung Han, Learning Deconvolution Network for Semantic Segmentation, arXiv:1505.04366. <a href="https://arxiv.org/pdf/1505.04366">[Paper]</a> (7th ranked in VOC2012)</li>
<li>Seunghoon Hong, Hyeonwoo Noh, Bohyung Han, Decoupled Deep Neural Network for Semi-supervised Semantic Segmentation, arXiv:1506.04924. <a href="https://arxiv.org/pdf/1506.04924">[Paper]</a></li>
<li>Seunghoon Hong,Junhyuk Oh,	Bohyung Han, and	Honglak Lee, Learning Transferrable Knowledge for Semantic Segmentation with Deep Convolutional Neural Network, arXiv:1512.07928 [<a href="https://arxiv.org/pdf/1512.07928.pdf">Paper</a>] [<a href="http://cvlab.postech.ac.kr/research/transfernet/">Project Page</a>]</li>
</ul>
</li>
<li>Conditional Random Fields as Recurrent Neural Networks <a href="https://arxiv.org/pdf/1502.03240">[Paper]</a>
<ul>
<li>Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-Paredes, Vibhav Vineet, Zhizhong Su, Dalong Du, Chang Huang, Philip H. S. Torr, Conditional Random Fields as Recurrent Neural Networks, arXiv:1502.03240. (8th ranked in VOC2012)</li>
</ul>
</li>
<li>DeepLab
<ul>
<li>Liang-Chieh Chen, George Papandreou, Kevin Murphy, Alan L. Yuille, Weakly-and semi-supervised learning of a DCNN for semantic image segmentation, arXiv:1502.02734. <a href="https://arxiv.org/pdf/1502.02734">[Paper]</a> (9th ranked in VOC2012)</li>
</ul>
</li>
<li>Zoom-out <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Mostajabi_Feedforward_Semantic_Segmentation_2015_CVPR_paper.pdf">[Paper]</a>
<ul>
<li>Mohammadreza Mostajabi, Payman Yadollahpour, Gregory Shakhnarovich, Feedforward Semantic Segmentation With Zoom-Out Features, CVPR, 2015</li>
</ul>
</li>
<li>Joint Calibration <a href="https://arxiv.org/pdf/1507.01581">[Paper]</a>
<ul>
<li>Holger Caesar, Jasper Uijlings, Vittorio Ferrari, Joint Calibration for Semantic Segmentation, arXiv:1507.01581.</li>
</ul>
</li>
<li>Fully Convolutional Networks for Semantic Segmentation <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf">[Paper-CVPR15]</a> <a href="https://arxiv.org/pdf/1411.4038">[Paper-arXiv15]</a>
<ul>
<li>Jonathan Long, Evan Shelhamer, Trevor Darrell, Fully Convolutional Networks for Semantic Segmentation, CVPR, 2015.</li>
</ul>
</li>
<li>Hypercolumn <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Hariharan_Hypercolumns_for_Object_2015_CVPR_paper.pdf">[Paper]</a>
<ul>
<li>Bharath Hariharan, Pablo Arbelaez, Ross Girshick, Jitendra Malik, Hypercolumns for Object Segmentation and Fine-Grained Localization, CVPR, 2015.</li>
</ul>
</li>
<li>Deep Hierarchical Parsing
<ul>
<li>Abhishek Sharma, Oncel Tuzel, David W. Jacobs, Deep Hierarchical Parsing for Semantic Segmentation, CVPR, 2015. <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Sharma_Deep_Hierarchical_Parsing_2015_CVPR_paper.pdf">[Paper]</a></li>
</ul>
</li>
<li>Learning Hierarchical Features for Scene Labeling <a href="http://yann.lecun.com/exdb/publis/pdf/farabet-icml-12.pdf">[Paper-ICML12]</a> <a href="http://yann.lecun.com/exdb/publis/pdf/farabet-pami-13.pdf">[Paper-PAMI13]</a>
<ul>
<li>Clement Farabet, Camille Couprie, Laurent Najman, Yann LeCun, Scene Parsing with Multiscale Feature Learning, Purity Trees, and Optimal Covers, ICML, 2012.</li>
<li>Clement Farabet, Camille Couprie, Laurent Najman, Yann LeCun, Learning Hierarchical Features for Scene Labeling, PAMI, 2013.</li>
</ul>
</li>
<li>University of Cambridge <a href="http://mi.eng.cam.ac.uk/projects/segnet/">[Web]</a>
<ul>
<li>Vijay Badrinarayanan, Alex Kendall and Roberto Cipolla “SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation.” arXiv preprint arXiv:1511.00561, 2015. <a href="https://arxiv.org/abs/1511.00561">[Paper]</a></li>
</ul>
</li>
<li>Alex Kendall, Vijay Badrinarayanan and Roberto Cipolla “Bayesian SegNet: Model Uncertainty in Deep Convolutional Encoder-Decoder Architectures for Scene Understanding.” arXiv preprint arXiv:1511.02680, 2015. <a href="https://arxiv.org/abs/1511.00561">[Paper]</a></li>
<li>Princeton
<ul>
<li>Fisher Yu, Vladlen Koltun, “Multi-Scale Context Aggregation by Dilated Convolutions”, ICLR 2016, [<a href="https://arxiv.org/pdf/1511.07122v2.pdf">Paper</a>]</li>
</ul>
</li>
<li>Univ. of Washington, Allen AI
<ul>
<li>Hamid Izadinia, Fereshteh Sadeghi, Santosh Kumar Divvala, Yejin Choi, Ali Farhadi, “Segment-Phrase Table for Semantic Segmentation, Visual Entailment and Paraphrasing”, ICCV, 2015, [<a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Izadinia_Segment-Phrase_Table_for_ICCV_2015_paper.pdf">Paper</a>]</li>
</ul>
</li>
<li>INRIA
<ul>
<li>Iasonas Kokkinos, “Pusing the Boundaries of Boundary Detection Using deep Learning”, ICLR 2016, [<a href="https://arxiv.org/pdf/1511.07386v2.pdf">Paper</a>]</li>
</ul>
</li>
<li>UCSB
<ul>
<li>Niloufar Pourian, S. Karthikeyan, and B.S. Manjunath, “Weakly supervised graph based semantic segmentation by learning communities of image-parts”, ICCV, 2015, [<a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Pourian_Weakly_Supervised_Graph_ICCV_2015_paper.pdf">Paper</a>]</li>
</ul>
</li>
</ul>
<h3 id="visual-attention-and-saliency">Visual Attention and Saliency</h3>
<p><img src="https://cloud.githubusercontent.com/assets/5226447/8492362/7ec65b88-2183-11e5-978f-017e45ddba32.png" alt="saliency" />
(from Nian Liu, Junwei Han, Dingwen Zhang, Shifeng Wen, Tianming Liu, Predicting Eye Fixations using Convolutional Neural Networks, CVPR, 2015.)</p>
<ul>
<li>Mr-CNN <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Liu_Predicting_Eye_Fixations_2015_CVPR_paper.pdf">[Paper]</a>
<ul>
<li>Nian Liu, Junwei Han, Dingwen Zhang, Shifeng Wen, Tianming Liu, Predicting Eye Fixations using Convolutional Neural Networks, CVPR, 2015.</li>
</ul>
</li>
<li>Learning a Sequential Search for Landmarks <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Singh_Learning_a_Sequential_2015_CVPR_paper.pdf">[Paper]</a>
<ul>
<li>Saurabh Singh, Derek Hoiem, David Forsyth, Learning a Sequential Search for Landmarks, CVPR, 2015.</li>
</ul>
</li>
<li>Multiple Object Recognition with Visual Attention <a href="https://arxiv.org/pdf/1412.7755.pdf">[Paper]</a>
<ul>
<li>Jimmy Lei Ba, Volodymyr Mnih, Koray Kavukcuoglu, Multiple Object Recognition with Visual Attention, ICLR, 2015.</li>
</ul>
</li>
<li>Recurrent Models of Visual Attention <a href="http://papers.nips.cc/paper/5542-recurrent-models-of-visual-attention.pdf">[Paper]</a>
<ul>
<li>Volodymyr Mnih, Nicolas Heess, Alex Graves, Koray Kavukcuoglu, Recurrent Models of Visual Attention, NIPS, 2014.</li>
</ul>
</li>
</ul>
<h3 id="object-recognition">Object Recognition</h3>
<ul>
<li>Weakly-supervised learning with convolutional neural networks <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Oquab_Is_Object_Localization_2015_CVPR_paper.pdf">[Paper]</a>
<ul>
<li>Maxime Oquab, Leon Bottou, Ivan Laptev, Josef Sivic, Is object localization for free? – Weakly-supervised learning with convolutional neural networks, CVPR, 2015.</li>
</ul>
</li>
<li>FV-CNN <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Cimpoi_Deep_Filter_Banks_2015_CVPR_paper.pdf">[Paper]</a>
<ul>
<li>Mircea Cimpoi, Subhransu Maji, Andrea Vedaldi, Deep Filter Banks for Texture Recognition and Segmentation, CVPR, 2015.</li>
</ul>
</li>
</ul>
<h3 id="human-pose-estimation">Human Pose Estimation</h3>
<ul>
<li>Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh, Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields, CVPR, 2017.</li>
<li>Leonid Pishchulin, Eldar Insafutdinov, Siyu Tang, Bjoern Andres, Mykhaylo Andriluka, Peter Gehler, and Bernt Schiele, Deepcut: Joint subset partition and labeling for multi person pose estimation, CVPR, 2016.</li>
<li>Shih-En Wei, Varun Ramakrishna, Takeo Kanade, and Yaser Sheikh, Convolutional pose machines, CVPR, 2016.</li>
<li>Alejandro Newell, Kaiyu Yang, and Jia Deng, Stacked hourglass networks for human pose estimation, ECCV, 2016.</li>
<li>Tomas Pfister, James Charles, and Andrew Zisserman, Flowing convnets for human pose estimation in videos, ICCV, 2015.</li>
<li>Jonathan J. Tompson, Arjun Jain, Yann LeCun, Christoph Bregler, Joint training of a convolutional network and a graphical model for human pose estimation, NIPS, 2014.</li>
</ul>
<h3 id="understanding-cnn">Understanding CNN</h3>
<p><img src="https://cloud.githubusercontent.com/assets/5226447/8452083/1aaa0066-2023-11e5-800b-2248ead51584.PNG" alt="understanding" />
(from Aravindh Mahendran, Andrea Vedaldi, Understanding Deep Image Representations by Inverting Them, CVPR, 2015.)</p>
<ul>
<li>Karel Lenc, Andrea Vedaldi, Understanding image representations by measuring their equivariance and equivalence, CVPR, 2015. <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Lenc_Understanding_Image_Representations_2015_CVPR_paper.pdf">[Paper]</a></li>
<li>Anh Nguyen, Jason Yosinski, Jeff Clune, Deep Neural Networks are Easily Fooled:High Confidence Predictions for Unrecognizable Images, CVPR, 2015. <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Nguyen_Deep_Neural_Networks_2015_CVPR_paper.pdf">[Paper]</a></li>
<li>Aravindh Mahendran, Andrea Vedaldi, Understanding Deep Image Representations by Inverting Them, CVPR, 2015. <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Mahendran_Understanding_Deep_Image_2015_CVPR_paper.pdf">[Paper]</a></li>
<li>Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, Antonio Torralba, Object Detectors Emerge in Deep Scene CNNs, ICLR, 2015. <a href="https://arxiv.org/abs/1412.6856">[arXiv Paper]</a></li>
<li>Alexey Dosovitskiy, Thomas Brox, Inverting Visual Representations with Convolutional Networks, arXiv, 2015. <a href="https://arxiv.org/abs/1506.02753">[Paper]</a></li>
<li>Matthrew Zeiler, Rob Fergus, Visualizing and Understanding Convolutional Networks, ECCV, 2014. <a href="https://www.cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf">[Paper]</a></li>
</ul>
<h3 id="image-and-language">Image and Language</h3>
<h4 id="image-captioning">Image Captioning</h4>
<p><img src="https://cloud.githubusercontent.com/assets/5226447/8452051/e8f81030-2022-11e5-85db-c68e7d8251ce.PNG" alt="image_captioning" />
(from Andrej Karpathy, Li Fei-Fei, Deep Visual-Semantic Alignments for Generating Image Description, CVPR, 2015.)</p>
<ul>
<li>UCLA / Baidu <a href="https://arxiv.org/pdf/1410.1090">[Paper]</a>
<ul>
<li>Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Alan L. Yuille, Explain Images with Multimodal Recurrent Neural Networks, arXiv:1410.1090.</li>
</ul>
</li>
<li>Toronto <a href="https://arxiv.org/pdf/1411.2539">[Paper]</a>
<ul>
<li>Ryan Kiros, Ruslan Salakhutdinov, Richard S. Zemel, Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models, arXiv:1411.2539.</li>
</ul>
</li>
<li>Berkeley <a href="https://arxiv.org/pdf/1411.4389">[Paper]</a>
<ul>
<li>Jeff Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, Trevor Darrell, Long-term Recurrent Convolutional Networks for Visual Recognition and Description, arXiv:1411.4389.</li>
</ul>
</li>
<li>Google <a href="https://arxiv.org/pdf/1411.4555">[Paper]</a>
<ul>
<li>Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan, Show and Tell: A Neural Image Caption Generator, arXiv:1411.4555.</li>
</ul>
</li>
<li>Stanford <a href="http://cs.stanford.edu/people/karpathy/deepimagesent/">[Web]</a> <a href="http://cs.stanford.edu/people/karpathy/cvpr2015.pdf">[Paper]</a>
<ul>
<li>Andrej Karpathy, Li Fei-Fei, Deep Visual-Semantic Alignments for Generating Image Description, CVPR, 2015.</li>
</ul>
</li>
<li>UML / UT <a href="https://arxiv.org/pdf/1412.4729">[Paper]</a>
<ul>
<li>Subhashini Venugopalan, Huijuan Xu, Jeff Donahue, Marcus Rohrbach, Raymond Mooney, Kate Saenko, Translating Videos to Natural Language Using Deep Recurrent Neural Networks, NAACL-HLT, 2015.</li>
</ul>
</li>
<li>CMU / Microsoft <a href="https://arxiv.org/pdf/1411.5654">[Paper-arXiv]</a> <a href="https://www.cs.cmu.edu/~xinleic/papers/cvpr15_rnn.pdf">[Paper-CVPR]</a>
<ul>
<li>Xinlei Chen, C. Lawrence Zitnick, Learning a Recurrent Visual Representation for Image Caption Generation, arXiv:1411.5654.</li>
<li>Xinlei Chen, C. Lawrence Zitnick, Mind’s Eye: A Recurrent Visual Representation for Image Caption Generation, CVPR 2015</li>
</ul>
</li>
<li>Microsoft <a href="https://arxiv.org/pdf/1411.4952">[Paper]</a>
<ul>
<li>Hao Fang, Saurabh Gupta, Forrest Iandola, Rupesh Srivastava, Li Deng, Piotr Dollár, Jianfeng Gao, Xiaodong He, Margaret Mitchell, John C. Platt, C. Lawrence Zitnick, Geoffrey Zweig, From Captions to Visual Concepts and Back, CVPR, 2015.</li>
</ul>
</li>
<li>Univ. Montreal / Univ. Toronto [<a href="https://kelvinxu.github.io/projects/capgen.html">Web</a>] [<a href="http://www.cs.toronto.edu/~zemel/documents/captionAttn.pdf">Paper</a>]
<ul>
<li>Kelvin Xu, Jimmy Lei Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard S. Zemel, Yoshua Bengio, Show, Attend, and Tell: Neural Image Caption Generation with Visual Attention, arXiv:1502.03044 / ICML 2015</li>
</ul>
</li>
<li>Idiap / EPFL / Facebook [<a href="https://arxiv.org/pdf/1502.03671">Paper</a>]
<ul>
<li>Remi Lebret, Pedro O. Pinheiro, Ronan Collobert, Phrase-based Image Captioning, arXiv:1502.03671 / ICML 2015</li>
</ul>
</li>
<li>UCLA / Baidu [<a href="https://arxiv.org/pdf/1504.06692">Paper</a>]
<ul>
<li>Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng Huang, Alan L. Yuille, Learning like a Child: Fast Novel Visual Concept Learning from Sentence Descriptions of Images, arXiv:1504.06692</li>
</ul>
</li>
<li>MS + Berkeley
<ul>
<li>Jacob Devlin, Saurabh Gupta, Ross Girshick, Margaret Mitchell, C. Lawrence Zitnick, Exploring Nearest Neighbor Approaches for Image Captioning, arXiv:1505.04467 [<a href="https://arxiv.org/pdf/1505.04467.pdf">Paper</a>]</li>
<li>Jacob Devlin, Hao Cheng, Hao Fang, Saurabh Gupta, Li Deng, Xiaodong He, Geoffrey Zweig, Margaret Mitchell, Language Models for Image Captioning: The Quirks and What Works, arXiv:1505.01809 [<a href="https://arxiv.org/pdf/1505.01809.pdf">Paper</a>]</li>
</ul>
</li>
<li>Adelaide [<a href="https://arxiv.org/pdf/1506.01144.pdf">Paper</a>]
<ul>
<li>Qi Wu, Chunhua Shen, Anton van den Hengel, Lingqiao Liu, Anthony Dick, Image Captioning with an Intermediate Attributes Layer, arXiv:1506.01144</li>
</ul>
</li>
<li>Tilburg [<a href="https://arxiv.org/pdf/1506.03694.pdf">Paper</a>]
<ul>
<li>Grzegorz Chrupala, Akos Kadar, Afra Alishahi, Learning language through pictures, arXiv:1506.03694</li>
</ul>
</li>
<li>Univ. Montreal [<a href="https://arxiv.org/pdf/1507.01053.pdf">Paper</a>]
<ul>
<li>Kyunghyun Cho, Aaron Courville, Yoshua Bengio, Describing Multimedia Content using Attention-based Encoder-Decoder Networks, arXiv:1507.01053</li>
</ul>
</li>
<li>Cornell [<a href="https://arxiv.org/pdf/1508.02091.pdf">Paper</a>]
<ul>
<li>Jack Hessel, Nicolas Savva, Michael J. Wilber, Image Representations and New Domains in Neural Image Captioning, arXiv:1508.02091</li>
</ul>
</li>
<li>MS + City Univ. of HongKong [<a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Yao_Learning_Query_and_ICCV_2015_paper.pdf">Paper</a>]
<ul>
<li>Ting Yao, Tao Mei, and Chong-Wah Ngo, “Learning Query and Image Similarities
with Ranking Canonical Correlation Analysis”, ICCV, 2015</li>
</ul>
</li>
</ul>
<h4 id="video-captioning">Video Captioning</h4>
<ul>
<li>Berkeley <a href="http://jeffdonahue.com/lrcn/">[Web]</a> <a href="https://arxiv.org/pdf/1411.4389.pdf">[Paper]</a>
<ul>
<li>Jeff Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, Trevor Darrell, Long-term Recurrent Convolutional Networks for Visual Recognition and Description, CVPR, 2015.</li>
</ul>
</li>
<li>UT / UML / Berkeley <a href="https://arxiv.org/pdf/1412.4729">[Paper]</a>
<ul>
<li>Subhashini Venugopalan, Huijuan Xu, Jeff Donahue, Marcus Rohrbach, Raymond Mooney, Kate Saenko, Translating Videos to Natural Language Using Deep Recurrent Neural Networks, arXiv:1412.4729.</li>
</ul>
</li>
<li>Microsoft <a href="https://arxiv.org/pdf/1505.01861">[Paper]</a>
<ul>
<li>Yingwei Pan, Tao Mei, Ting Yao, Houqiang Li, Yong Rui, Joint Modeling Embedding and Translation to Bridge Video and Language, arXiv:1505.01861.</li>
</ul>
</li>
<li>UT / Berkeley / UML <a href="https://arxiv.org/pdf/1505.00487">[Paper]</a>
<ul>
<li>Subhashini Venugopalan, Marcus Rohrbach, Jeff Donahue, Raymond Mooney, Trevor Darrell, Kate Saenko, Sequence to Sequence–Video to Text, arXiv:1505.00487.</li>
</ul>
</li>
<li>Univ. Montreal / Univ. Sherbrooke [<a href="https://arxiv.org/pdf/1502.08029.pdf">Paper</a>]
<ul>
<li>Li Yao, Atousa Torabi, Kyunghyun Cho, Nicolas Ballas, Christopher Pal, Hugo Larochelle, Aaron Courville, Describing Videos by Exploiting Temporal Structure, arXiv:1502.08029</li>
</ul>
</li>
<li>MPI / Berkeley [<a href="https://arxiv.org/pdf/1506.01698.pdf">Paper</a>]
<ul>
<li>Anna Rohrbach, Marcus Rohrbach, Bernt Schiele, The Long-Short Story of Movie Description, arXiv:1506.01698</li>
</ul>
</li>
<li>Univ. Toronto / MIT [<a href="https://arxiv.org/pdf/1506.06724.pdf">Paper</a>]
<ul>
<li>Yukun Zhu, Ryan Kiros, Richard Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, Sanja Fidler, Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books, arXiv:1506.06724</li>
</ul>
</li>
<li>Univ. Montreal [<a href="https://arxiv.org/pdf/1507.01053.pdf">Paper</a>]
<ul>
<li>Kyunghyun Cho, Aaron Courville, Yoshua Bengio, Describing Multimedia Content using Attention-based Encoder-Decoder Networks, arXiv:1507.01053</li>
</ul>
</li>
<li>TAU / USC [<a href="https://arxiv.org/pdf/1612.06950.pdf">paper</a>]
<ul>
<li>Dotan Kaufman, Gil Levi, Tal Hassner, Lior Wolf, Temporal Tessellation for Video Annotation and Summarization, arXiv:1612.06950.</li>
</ul>
</li>
</ul>
<h4 id="question-answering">Question Answering</h4>
<p><img src="https://cloud.githubusercontent.com/assets/5226447/8452068/ffe7b1f6-2022-11e5-87ab-4f6d4696c220.PNG" alt="question_answering" />
(from Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, Devi Parikh, VQA: Visual Question Answering, CVPR, 2015 SUNw:Scene Understanding workshop)</p>
<ul>
<li>Virginia Tech / MSR <a href="http://www.visualqa.org/">[Web]</a> <a href="https://arxiv.org/pdf/1505.00468">[Paper]</a>
<ul>
<li>Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, Devi Parikh, VQA: Visual Question Answering, CVPR, 2015 SUNw:Scene Understanding workshop.</li>
</ul>
</li>
<li>MPI / Berkeley <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/vision-and-language/visual-turing-challenge/">[Web]</a> <a href="https://arxiv.org/pdf/1505.01121">[Paper]</a>
<ul>
<li>Mateusz Malinowski, Marcus Rohrbach, Mario Fritz, Ask Your Neurons: A Neural-based Approach to Answering Questions about Images, arXiv:1505.01121.</li>
</ul>
</li>
<li>Toronto <a href="https://arxiv.org/pdf/1505.02074">[Paper]</a> <a href="http://www.cs.toronto.edu/~mren/imageqa/data/cocoqa/">[Dataset]</a>
<ul>
<li>Mengye Ren, Ryan Kiros, Richard Zemel, Image Question Answering: A Visual Semantic Embedding Model and a New Dataset, arXiv:1505.02074 / ICML 2015 deep learning workshop.</li>
</ul>
</li>
<li>Baidu / UCLA <a href="https://arxiv.org/pdf/1505.05612">[Paper]</a> <a href="">[Dataset]</a>
<ul>
<li>Hauyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, Wei Xu, Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering, arXiv:1505.05612.</li>
</ul>
</li>
<li>POSTECH [<a href="https://arxiv.org/pdf/1511.05756.pdf">Paper</a>] [<a href="http://cvlab.postech.ac.kr/research/dppnet/">Project Page</a>]
<ul>
<li>Hyeonwoo Noh, Paul Hongsuck Seo, and Bohyung Han, Image Question Answering using Convolutional Neural Network with Dynamic Parameter Prediction, arXiv:1511.05765</li>
</ul>
</li>
<li>CMU / Microsoft Research [<a href="https://arxiv.org/pdf/1511.02274v2.pdf">Paper</a>]
<ul>
<li>Yang, Z., He, X., Gao, J., Deng, L., &amp; Smola, A. (2015). Stacked Attention Networks for Image Question Answering. arXiv:1511.02274.</li>
</ul>
</li>
<li>MetaMind [<a href="https://arxiv.org/pdf/1603.01417v1.pdf">Paper</a>]
<ul>
<li>Xiong, Caiming, Stephen Merity, and Richard Socher. “Dynamic Memory Networks for Visual and Textual Question Answering.” arXiv:1603.01417 (2016).</li>
</ul>
</li>
<li>SNU + NAVER [<a href="https://arxiv.org/abs/1606.01455">Paper</a>]
<ul>
<li>Jin-Hwa Kim, Sang-Woo Lee, Dong-Hyun Kwak, Min-Oh Heo, Jeonghee Kim, Jung-Woo Ha, Byoung-Tak Zhang, <em>Multimodal Residual Learning for Visual QA</em>, arXiv:1606:01455</li>
</ul>
</li>
<li>UC Berkeley + Sony [<a href="https://arxiv.org/pdf/1606.01847">Paper</a>]
<ul>
<li>Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and Marcus Rohrbach, <em>Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding</em>, arXiv:1606.01847</li>
</ul>
</li>
<li>Postech [<a href="https://arxiv.org/pdf/1606.03647.pdf">Paper</a>]
<ul>
<li>Hyeonwoo Noh and Bohyung Han, <em>Training Recurrent Answering Units with Joint Loss Minimization for VQA</em>, arXiv:1606.03647</li>
</ul>
</li>
<li>SNU + NAVER [<a href="https://arxiv.org/abs/1610.04325">Paper</a>]
<ul>
<li>Jin-Hwa Kim, Kyoung Woon On, Jeonghee Kim, Jung-Woo Ha, Byoung-Tak Zhang, <em>Hadamard Product for Low-rank Bilinear Pooling</em>, arXiv:1610.04325.</li>
</ul>
</li>
</ul>
<h3 id="image-generation">Image Generation</h3>
<ul>
<li>Convolutional / Recurrent Networks
<ul>
<li>Aäron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, Koray Kavukcuoglu. “Conditional Image Generation with PixelCNN Decoders”<a href="https://arxiv.org/pdf/1606.05328v2.pdf">[Paper]</a><a href="https://github.com/kundan2510/pixelCNN">[Code]</a></li>
<li>Alexey Dosovitskiy, Jost Tobias Springenberg, Thomas Brox, “Learning to Generate Chairs with Convolutional Neural Networks”, CVPR, 2015. <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Dosovitskiy_Learning_to_Generate_2015_CVPR_paper.pdf">[Paper]</a></li>
<li>Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende, Daan Wierstra, “DRAW: A Recurrent Neural Network For Image Generation”, ICML, 2015. [<a href="https://arxiv.org/pdf/1502.04623v2.pdf">Paper</a>]</li>
</ul>
</li>
<li>Adversarial Networks
<ul>
<li>Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, Generative Adversarial Networks, NIPS, 2014. <a href="https://arxiv.org/abs/1406.2661">[Paper]</a></li>
<li>Emily Denton, Soumith Chintala, Arthur Szlam, Rob Fergus, Deep Generative Image Models using a ￼Laplacian Pyramid of Adversarial Networks, NIPS, 2015. <a href="https://arxiv.org/abs/1506.05751">[Paper]</a></li>
<li>Lucas Theis, Aäron van den Oord, Matthias Bethge, “A note on the evaluation of generative models”, ICLR 2016. [<a href="https://arxiv.org/abs/1511.01844">Paper</a>]</li>
<li>Zhenwen Dai, Andreas Damianou, Javier Gonzalez, Neil Lawrence, “Variationally Auto-Encoded Deep Gaussian Processes”, ICLR 2016. [<a href="https://arxiv.org/pdf/1511.06455v2.pdf">Paper</a>]</li>
<li>Elman Mansimov, Emilio Parisotto, Jimmy Ba, Ruslan Salakhutdinov, “Generating Images from Captions with Attention”, ICLR 2016, [<a href="https://arxiv.org/pdf/1511.02793v2.pdf">Paper</a>]</li>
<li>Jost Tobias Springenberg, “Unsupervised and Semi-supervised Learning with Categorical Generative Adversarial Networks”, ICLR 2016, [<a href="https://arxiv.org/pdf/1511.06390v1.pdf">Paper</a>]</li>
<li>Harrison Edwards, Amos Storkey, “Censoring Representations with an Adversary”, ICLR 2016, [<a href="https://arxiv.org/pdf/1511.05897v3.pdf">Paper</a>]</li>
<li>Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, Ken Nakae, Shin Ishii, “Distributional Smoothing with Virtual Adversarial Training”, ICLR 2016, [<a href="https://arxiv.org/pdf/1507.00677v8.pdf">Paper</a>]</li>
<li>Jun-Yan Zhu, Philipp Krahenbuhl, Eli Shechtman, and Alexei A. Efros, “Generative Visual Manipulation on the Natural Image Manifold”, ECCV 2016. [<a href="https://arxiv.org/pdf/1609.03552v2.pdf">Paper</a>] [<a href="https://github.com/junyanz/iGAN">Code</a>] [<a href="https://youtu.be/9c4z6YsBGQ0">Video</a>]</li>
</ul>
</li>
<li>Mixing Convolutional and Adversarial Networks
<ul>
<li>Alec Radford, Luke Metz, Soumith Chintala, “Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks”, ICLR 2016. [<a href="https://arxiv.org/pdf/1511.06434.pdf">Paper</a>]</li>
</ul>
</li>
</ul>
<h3 id="other-topics">Other Topics</h3>
<ul>
<li>Visual Analogy [<a href="https://web.eecs.umich.edu/~honglak/nips2015-analogy.pdf">Paper</a>]
<ul>
<li>Scott Reed, Yi Zhang, Yuting Zhang, Honglak Lee, Deep Visual Analogy Making, NIPS, 2015</li>
</ul>
</li>
<li>Surface Normal Estimation <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Wang_Designing_Deep_Networks_2015_CVPR_paper.pdf">[Paper]</a>
<ul>
<li>Xiaolong Wang, David F. Fouhey, Abhinav Gupta, Designing Deep Networks for Surface Normal Estimation, CVPR, 2015.</li>
</ul>
</li>
<li>Action Detection <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Gkioxari_Finding_Action_Tubes_2015_CVPR_paper.pdf">[Paper]</a>
<ul>
<li>Georgia Gkioxari, Jitendra Malik, Finding Action Tubes, CVPR, 2015.</li>
</ul>
</li>
<li>Crowd Counting <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Zhang_Cross-Scene_Crowd_Counting_2015_CVPR_paper.pdf">[Paper]</a>
<ul>
<li>Cong Zhang, Hongsheng Li, Xiaogang Wang, Xiaokang Yang, Cross-scene Crowd Counting via Deep Convolutional Neural Networks, CVPR, 2015.</li>
</ul>
</li>
<li>3D Shape Retrieval <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Wang_Sketch-Based_3D_Shape_2015_CVPR_paper.pdf">[Paper]</a>
<ul>
<li>Fang Wang, Le Kang, Yi Li, Sketch-based 3D Shape Retrieval using Convolutional Neural Networks, CVPR, 2015.</li>
</ul>
</li>
<li>Weakly-supervised Classification
<ul>
<li>Samaneh Azadi, Jiashi Feng, Stefanie Jegelka, Trevor Darrell, “Auxiliary Image Regularization for Deep CNNs with Noisy Labels”, ICLR 2016, [<a href="https://arxiv.org/pdf/1511.07069v2.pdf">Paper</a>]</li>
</ul>
</li>
<li>Artistic Style <a href="https://arxiv.org/abs/1508.06576">[Paper]</a> <a href="https://github.com/jcjohnson/neural-style">[Code]</a>
<ul>
<li>Leon A. Gatys, Alexander S. Ecker, Matthias Bethge, A Neural Algorithm of Artistic Style.</li>
</ul>
</li>
<li>Human Gaze Estimation
<ul>
<li>Xucong Zhang, Yusuke Sugano, Mario Fritz, Andreas Bulling, Appearance-Based Gaze Estimation in the Wild, CVPR, 2015. <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Zhang_Appearance-Based_Gaze_Estimation_2015_CVPR_paper.pdf">[Paper]</a> <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/gaze-based-human-computer-interaction/appearance-based-gaze-estimation-in-the-wild-mpiigaze/">[Website]</a></li>
</ul>
</li>
<li>Face Recognition
<ul>
<li>Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, Lior Wolf, DeepFace: Closing the Gap to Human-Level Performance in Face Verification, CVPR, 2014. <a href="https://www.cs.toronto.edu/~ranzato/publications/taigman_cvpr14.pdf">[Paper]</a></li>
<li>Yi Sun, Ding Liang, Xiaogang Wang, Xiaoou Tang, DeepID3: Face Recognition with Very Deep Neural Networks, 2015. <a href="https://arxiv.org/abs/1502.00873">[Paper]</a></li>
<li>Florian Schroff, Dmitry Kalenichenko, James Philbin, FaceNet: A Unified Embedding for Face Recognition and Clustering, CVPR, 2015. <a href="https://arxiv.org/abs/1503.03832">[Paper]</a></li>
</ul>
</li>
<li>Facial Landmark Detection
<ul>
<li>Yue Wu, Tal Hassner, KangGeon Kim, Gerard Medioni, Prem Natarajan, Facial Landmark Detection with Tweaked Convolutional Neural Networks, 2015. <a href="https://arxiv.org/abs/1511.04031">[Paper]</a> <a href="http://www.openu.ac.il/home/hassner/projects/tcnn_landmarks/">[Project]</a></li>
</ul>
</li>
</ul>
<h2 id="courses">Courses</h2>
<ul>
<li>Deep Vision
<ul>
<li>[Stanford] <a href="http://cs231n.stanford.edu/">CS231n: Convolutional Neural Networks for Visual Recognition</a></li>
<li>[CUHK] <a href="https://piazza.com/cuhk.edu.hk/spring2015/eleg5040/home">ELEG 5040: Advanced Topics in Signal Processing(Introduction to Deep Learning)</a></li>
</ul>
</li>
<li>More Deep Learning
<ul>
<li>[Stanford] <a href="http://cs224d.stanford.edu/">CS224d: Deep Learning for Natural Language Processing</a></li>
<li>[Oxford] <a href="https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/">Deep Learning by Prof. Nando de Freitas</a></li>
<li>[NYU] <a href="http://cilvr.cs.nyu.edu/doku.php?id=courses:deeplearning2014:start">Deep Learning by Prof. Yann LeCun</a></li>
</ul>
</li>
</ul>
<h2 id="books">Books</h2>
<ul>
<li>Free Online Books
<ul>
<li><a href="http://www.iro.umontreal.ca/~bengioy/dlbook/">Deep Learning by Ian Goodfellow, Yoshua Bengio, and Aaron Courville</a></li>
<li><a href="http://neuralnetworksanddeeplearning.com/">Neural Networks and Deep Learning by Michael Nielsen</a></li>
<li><a href="http://deeplearning.net/tutorial/deeplearning.pdf">Deep Learning Tutorial by LISA lab, University of Montreal</a></li>
</ul>
</li>
</ul>
<h2 id="videos">Videos</h2>
<ul>
<li>Talks
<ul>
<li><a href="https://www.youtube.com/watch?v=n1ViNeWhC24">Deep Learning, Self-Taught Learning and Unsupervised Feature Learning By Andrew Ng</a></li>
<li><a href="https://www.youtube.com/watch?v=vShMxxqtDDs">Recent Developments in Deep Learning By Geoff Hinton</a></li>
<li><a href="https://www.youtube.com/watch?v=sc-KbuZqGkI">The Unreasonable Effectiveness of Deep Learning by Yann LeCun</a></li>
<li><a href="https://www.youtube.com/watch?v=4xsVFLnHC_0">Deep Learning of Representations by Yoshua bengio</a></li>
</ul>
</li>
</ul>
<h2 id="software">Software</h2>
<h3 id="framework">Framework</h3>
<ul>
<li>Tensorflow: An open source software library for numerical computation using data flow graph by Google [<a href="https://www.tensorflow.org/">Web</a>]</li>
<li>Torch7: Deep learning library in Lua, used by Facebook and Google Deepmind [<a href="http://torch.ch/">Web</a>]
<ul>
<li>Torch-based deep learning libraries: [<a href="https://github.com/torchnet/torchnet">torchnet</a>],</li>
</ul>
</li>
<li>Caffe: Deep learning framework by the BVLC [<a href="http://caffe.berkeleyvision.org/">Web</a>]</li>
<li>Theano: Mathematical library in Python, maintained by LISA lab [<a href="http://deeplearning.net/software/theano/">Web</a>]
<ul>
<li>Theano-based deep learning libraries: [<a href="http://deeplearning.net/software/pylearn2/">Pylearn2</a>], [<a href="https://github.com/mila-udem/blocks">Blocks</a>], [<a href="http://keras.io/">Keras</a>], [<a href="https://github.com/Lasagne/Lasagne">Lasagne</a>]</li>
</ul>
</li>
<li>MatConvNet: CNNs for MATLAB [<a href="http://www.vlfeat.org/matconvnet/">Web</a>]</li>
<li>MXNet: A flexible and efficient deep learning library for heterogeneous distributed systems with multi-language support [<a href="http://mxnet.io/">Web</a>]</li>
<li>Deepgaze: A computer vision library for human-computer interaction based on CNNs [<a href="https://github.com/mpatacchiola/deepgaze">Web</a>]</li>
</ul>
<h3 id="applications">Applications</h3>
<ul>
<li>Adversarial Training
<ul>
<li>Code and hyperparameters for the paper “Generative Adversarial Networks” <a href="https://github.com/goodfeli/adversarial">[Web]</a></li>
</ul>
</li>
<li>Understanding and Visualizing
<ul>
<li>Source code for “Understanding Deep Image Representations by Inverting Them,” CVPR, 2015. <a href="https://github.com/aravindhm/deep-goggle">[Web]</a></li>
</ul>
</li>
<li>Semantic Segmentation
<ul>
<li>Source code for the paper “Rich feature hierarchies for accurate object detection and semantic segmentation,” CVPR, 2014. <a href="https://github.com/rbgirshick/rcnn">[Web]</a></li>
<li>Source code for the paper “Fully Convolutional Networks for Semantic Segmentation,” CVPR, 2015. <a href="https://github.com/longjon/caffe/tree/future">[Web]</a></li>
</ul>
</li>
<li>Super-Resolution
<ul>
<li>Image Super-Resolution for Anime-Style-Art <a href="https://github.com/nagadomi/waifu2x">[Web]</a></li>
</ul>
</li>
<li>Edge Detection
<ul>
<li>Source code for the paper “DeepContour: A Deep Convolutional Feature Learned by Positive-Sharing Loss for Contour Detection,” CVPR, 2015. <a href="https://github.com/shenwei1231/DeepContour">[Web]</a></li>
<li>Source code for the paper “Holistically-Nested Edge Detection”, ICCV 2015. <a href="https://github.com/s9xie/hed">[Web]</a></li>
</ul>
</li>
</ul>
<h2 id="tutorials">Tutorials</h2>
<ul>
<li>[CVPR 2014] <a href="https://sites.google.com/site/deeplearningcvpr2014/">Tutorial on Deep Learning in Computer Vision</a></li>
<li>[CVPR 2015] <a href="https://github.com/soumith/cvpr2015">Applied Deep Learning for Computer Vision with Torch</a></li>
</ul>
<h2 id="blogs">Blogs</h2>
<ul>
<li><a href="http://www.computervisionblog.com/2015/06/deep-down-rabbit-hole-cvpr-2015-and.html">Deep down the rabbit hole: CVPR 2015 and <span class="__cf_email__" data-cfemail="95f7f0ecfafbf1d5c1faf8f7fafbf0">[email&#160;protected]</span>’s Computer Vision Blog</a></li>
<li><a href="https://zoyathinks.blogspot.kr/2015/06/cvpr-recap-and-where-were-going.html">CVPR recap and where we’re <span class="__cf_email__" data-cfemail="791e1610171e3923160018">[email&#160;protected]</span> Bylinskii (MIT PhD Student)’s Blog</a></li>
<li><a href="https://www.wired.com/2015/06/facebook-googles-fake-brains-spawn-new-visual-reality/">Facebook’s AI <span class="__cf_email__" data-cfemail="6b3b0a02051f02050c2b3c02190e0f">[email&#160;protected]</span></a></li>
<li><a href="https://googleresearch.blogspot.kr/2015/06/inceptionism-going-deeper-into-neural.html">Inceptionism: Going Deeper into Neural <span class="__cf_email__" data-cfemail="410f2435362e332a3201062e2e262d24">[email&#160;protected]</span> Research</a></li>
<li><a href="https://peterroelants.github.io/">Implementing Neural networks</a></li>
</ul>
</article>

</div>
</main>
<script data-cfasync="false" src="/cdn-cgi/scripts/d07b1474/cloudflare-static/email-decode.min.js"></script><script>
  ((window.gitter = {}).chat = {}).options = {
    room: 'deeplearning4j/deeplearning4j',
    activationElement: false
  };
</script>
<script src="https://sidecar.gitter.im/dist/sidecar.v1.js" async defer></script>
<div class="dl4j-gitter-open-chat-button js-gitter-toggle-chat-button animated rubberBand"><i class="fa fa-comments-o" aria-hidden="true"></i> Chat with us on Gitter</div>

<footer class="site-footer">
<div class="container">
<a id="scroll-up" href="#"><i class="fa fa-angle-up"></i></a>
<div class="row">
<div class="col-md-6 col-sm-6">
<ul class="footer-menu">
<li><a href="https://github.com/deeplearning4j/">Github</a></li>
<li><a href="https://twitter.com/deeplearning4j">Tweets</a></li>
<li><a href="https://www.facebook.com/deeplearning4j/">Facebook</a>
<li><a href="../cn/index">中文</a></li>
<li><a href="../ja-index">日本語</a></li>
<li><a href="../kr-index">한글</a></li>
<li><a href="http://nd4j.org/">ND4J</a></li>
</ul>
</div>
<div class="col-md-6 col-sm-6">
<p>Copyright &copy; 2017. <a href="https://www.skymind.io">Skymind</a>. DL4J is licensed Apache 2.0.</p>
</div>
</div>
</div>
</footer>


<script src="../assets/themes/thedocs/js/theDocs.all.min.js"></script>
<script src="../assets/themes/thedocs/js/theDocs.js"></script>


<script>
jQuery(function($) {
$(".sidenav a").filter(function() {
return this.href == window.location;
}).addClass('active').closest('ul').prev('a').click();
});
</script>

</body>
</html>
